{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penguins classification\n",
    "\n",
    "#### Authors: Marková, Pěstová, Pronevich, Sokol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Importing all needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "seed = 234"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading and understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../data/raw.csv\"\n",
    "\n",
    "try:\n",
    "    df_penguins = pd.read_csv(path_to_data)\n",
    "except:\n",
    "    df_penguins = pd.read_csv('https://raw.githubusercontent.com/hmarkova/DataX_2023/main/data/raw.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguins.describe(include=\"all\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguins.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be see, there are wrong data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguins = df_penguins.convert_dtypes()\n",
    "df_penguins[\"species\"] = df_penguins[\"species\"].astype(\"category\")\n",
    "df_penguins[\"island\"] = df_penguins[\"island\"].astype(\"category\")\n",
    "df_penguins[\"sex\"] = df_penguins[\"sex\"].astype(\"category\")\n",
    "df_penguins[\"year\"] = df_penguins[\"year\"].astype(\"category\")\n",
    "df_penguins.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some missing values and duplicated rows, therefore all of these rows will be dropped from the dataset.\n",
    "\n",
    "After dropping the rows new dataset has 377 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of dupliated rows: \",df_penguins.duplicated().sum())\n",
    "print(\"\\nNumber of missing values in each column: \")\n",
    "print(df_penguins.isna().sum())\n",
    "\n",
    "df_penguins = df_penguins.drop_duplicates()\n",
    "df_penguins = df_penguins.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "Distribution of the target variable Species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = df_penguins[\"species\"].value_counts()\n",
    "plt.bar(species.index, species.values , align = \"center\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of other categorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "island = df_penguins[\"island\"].value_counts()\n",
    "sex = df_penguins[\"sex\"].value_counts()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].bar(island.index, island.values, align=\"center\") \n",
    "axs[0].set_title(\"Island\")\n",
    "axs[0].set_ylabel(\"Count\")\n",
    "\n",
    "axs[1].bar(sex.index, sex.values, align=\"center\") \n",
    "axs[1].set_title(\"Sex\")\n",
    "axs[1].set_ylabel(\"Count\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportion of species on each island"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_penguins.groupby([\"island\", \"species\"]).size().unstack(fill_value=0)\n",
    "grouped.plot(kind=\"bar\", stacked=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms and scatter plosts of the continuous variable, all colored by type of the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_penguins, hue=\"species\",diag_kind=\"hist\",height=3)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also made a corellation matrix. At first glance we can see a very high positive dependence of the variables flipper_length_mm and body_mass_g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_penguins.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguins = pd.get_dummies(df_penguins, columns=[\"island\",\"sex\",\"year\"], drop_first=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguins.to_csv('../data/processed.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset into training (70%), validation (15%) and test (15%) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(df_penguins.drop('species', axis=1), df_penguins['species'], test_size=0.15, random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the continues data. The normalization scale was created on training data only to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = MinMaxScaler()\n",
    "norm.fit(X_train[[\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]])\n",
    "X_train[[\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]] = norm.transform(X_train[[\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]])\n",
    "X_test[[\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]] = norm.transform(X_test[[\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]])\n",
    "X_val[[\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]] = norm.transform(X_val[[\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving interim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interim = pd.DataFrame()\n",
    "df_interim = df_interim.append(X_train)\n",
    "df_interim = df_interim.merge(X_test, how='outer')\n",
    "df_interim = df_interim.merge(X_val, how='outer')\n",
    "df_interim.to_csv('../data/interim.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "We are going to use 3 models - Random Forest, Logistic Regression and Gradient Boosting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(random_state=seed)\n",
    "rf_clf = RandomForestClassifier(random_state=seed)\n",
    "gb_clf = GradientBoostingClassifier(random_state=seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model, we will tune its hyperparameters based on Bayesian Optimization. Also we use 5-fold Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_param_grid = {\n",
    "    'fit_intercept': Categorical([True, False]),\n",
    "    'C': Real(0.001, 1000),\n",
    "    'penalty': Categorical(['l2', 'none'])\n",
    "}\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': Integer(1, 1000),\n",
    "    'criterion': Categorical(['gini', 'entropy']),\n",
    "    'max_depth': Integer(1, 15),\n",
    "    'max_features': Integer(3, X_train.shape[1]),\n",
    "    'min_samples_leaf': Integer(5, 500)\n",
    "}\n",
    "\n",
    "gb_param_grid = {\n",
    "    'n_estimators': Integer(1, 1000),\n",
    "    'max_depth': Integer(1, 15),\n",
    "    'learning_rate': Real(0.001, 100),\n",
    "    'min_samples_leaf': Integer(5, 500),\n",
    "    'max_features': Integer(3, X_train.shape[1])\n",
    "}\n",
    "\n",
    "lr_search = BayesSearchCV(\n",
    "    estimator=lr_clf,\n",
    "    search_spaces=lr_param_grid,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    random_state=seed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_search = BayesSearchCV(\n",
    "    estimator=rf_clf,\n",
    "    search_spaces=rf_param_grid,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    random_state=seed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gb_search = BayesSearchCV(\n",
    "    estimator=gb_clf,\n",
    "    search_spaces=gb_param_grid,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    random_state=seed,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_search.fit(X_train, y_train)\n",
    "rf_search.fit(X_train, y_train)\n",
    "gb_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the models with best parametrs. In next step are pirnted the best models for each type of the model with it's best hyperparametrs and with score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression:')\n",
    "print('Best model:', lr_search.best_estimator_)\n",
    "print('Best score:', lr_search.best_score_)\n",
    "print('Random Forest:')\n",
    "print('Best model:', rf_search.best_estimator_)\n",
    "print('Best score:', rf_search.best_score_)\n",
    "print('Gradient Boosting:')\n",
    "print('Best model:', gb_search.best_estimator_)\n",
    "print('Best score:', gb_search.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally every of the best models is fitted to the validation data a is printed accuracy score for each of them. \n",
    "\n",
    "The best models are Logistics Regression and Gradient Boosting, which gave us 100% accuracy on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {\n",
    "    'Logistic Regression': lr_search.best_estimator_,\n",
    "    'Random Forest': rf_search.best_estimator_,\n",
    "    'Gradient Boosting': gb_search.best_estimator_\n",
    "}\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    model.fit(X_train_val, y_train_val)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(f'{name}: {score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of logistic regresion\n",
    "\n",
    "As can be seen below in the confusion matrix, our Logistic Regrassion model indeed classifies and distinguishes between every of the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confm = pd.DataFrame(confusion_matrix(y_val, lr_search.best_estimator_.predict(X_val))).rename(\n",
    "                                        columns = {0: 'Predicted - Adelie',1: 'Predicted - Chinstrap',2: 'Predicted - Gentoo'},\n",
    "                                        index = {0: 'Actual - Adelie',1: 'Actual - Chinstrap',2: 'Actual - Gentoo'})\n",
    "\n",
    "sns.heatmap(confm, annot = True, cmap ='Wistia', fmt = 'g')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
